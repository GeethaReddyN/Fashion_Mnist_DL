{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to FashionMNIST/processed/FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a0a221dbf5431c9b66c02a1bfd7d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/processed/FashionMNIST\\raw\\train-images-idx3-ubyte.gz to FashionMNIST/processed/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to FashionMNIST/processed/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bac63a3889461da9ab2f5207996e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/processed/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to FashionMNIST/processed/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to FashionMNIST/processed/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc77e5688e6841fd9d65f0634c61a71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/processed/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to FashionMNIST/processed/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to FashionMNIST/processed/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1aeff6f9463431ab849b3260d1446d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/processed/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to FashionMNIST/processed/FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\91832\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.RandomResizedCrop(24), transforms.ToTensor()])\n",
    "\n",
    "\n",
    "train = datasets.FashionMNIST(root=\"FashionMNIST/processed/\", train=True, download=True, transform=transform)\n",
    "test = datasets.FashionMNIST(root=\"FashionMNIST/processed/\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: FashionMNIST/processed/\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomResizedCrop(size=(24, 24), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "train_data = DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_data = DataLoader(test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_data))[0].view(32,-1).shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features=next(iter(train_data))[0].view(32,-1).shape[-1]\n",
    "in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,in_features):\n",
    "        super().__init__()\n",
    "        self.fc1=nn.Linear(in_features,128)\n",
    "        self.fc2=nn.Linear(128,128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4=nn.Linear(32,10)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.softmax=nn.Softmax()\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.relu(self.fc1(x))\n",
    "        x=self.dropout(x)\n",
    "        x=self.relu(self.fc2(x))\n",
    "        x=self.dropout(x)\n",
    "        x=self.relu(self.fc3(x))\n",
    "        x=self.dropout(x)\n",
    "        x=self.softmax(self.fc4(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Net(in_features)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-3ebabfa07fce>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x=self.softmax(self.fc4(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the epoch0/50\n",
      "________________________________________\n",
      "Training loss: 3808.296142578125\n",
      "Validation loss: 602.5248413085938\n",
      "Accuracy score: 0.5223\n",
      "\n",
      "Running the epoch1/50\n",
      "________________________________________\n",
      "Training loss: 3626.6826171875\n",
      "Validation loss: 595.496826171875\n",
      "Accuracy score: 0.5476\n",
      "\n",
      "Running the epoch2/50\n",
      "________________________________________\n",
      "Training loss: 3586.671142578125\n",
      "Validation loss: 587.8648681640625\n",
      "Accuracy score: 0.5748\n",
      "\n",
      "Running the epoch3/50\n",
      "________________________________________\n",
      "Training loss: 3565.688232421875\n",
      "Validation loss: 585.3892822265625\n",
      "Accuracy score: 0.5824\n",
      "\n",
      "Running the epoch4/50\n",
      "________________________________________\n",
      "Training loss: 3557.0419921875\n",
      "Validation loss: 586.276611328125\n",
      "Accuracy score: 0.5769\n",
      "\n",
      "Running the epoch5/50\n",
      "________________________________________\n",
      "Training loss: 3545.450927734375\n",
      "Validation loss: 581.3710327148438\n",
      "Accuracy score: 0.5928\n",
      "\n",
      "Running the epoch6/50\n",
      "________________________________________\n",
      "Training loss: 3533.331787109375\n",
      "Validation loss: 581.75341796875\n",
      "Accuracy score: 0.593\n",
      "\n",
      "Running the epoch7/50\n",
      "________________________________________\n",
      "Training loss: 3532.38134765625\n",
      "Validation loss: 579.8695068359375\n",
      "Accuracy score: 0.5982\n",
      "\n",
      "Running the epoch8/50\n",
      "________________________________________\n",
      "Training loss: 3519.161865234375\n",
      "Validation loss: 583.7843627929688\n",
      "Accuracy score: 0.5863\n",
      "\n",
      "Running the epoch9/50\n",
      "________________________________________\n",
      "Training loss: 3517.515380859375\n",
      "Validation loss: 578.9476318359375\n",
      "Accuracy score: 0.6006\n",
      "\n",
      "Running the epoch10/50\n",
      "________________________________________\n",
      "Training loss: 3509.072021484375\n",
      "Validation loss: 581.993408203125\n",
      "Accuracy score: 0.5919\n",
      "\n",
      "Running the epoch11/50\n",
      "________________________________________\n",
      "Training loss: 3511.50146484375\n",
      "Validation loss: 583.9432373046875\n",
      "Accuracy score: 0.5864\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-a308eca1a2e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#calculating the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#applying backward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#updating the parameters(Gradient Descent)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ranjan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ranjan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device=('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "training_loss_epoch=[]\n",
    "val_loss_epoch=[]\n",
    "\n",
    "best_weights=dict()\n",
    "\n",
    "epochs=50\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "#Training the model\n",
    "for i in range(epochs):\n",
    "    train_loss=0\n",
    "    val_loss=0\n",
    "    total_corrects=0\n",
    "    \n",
    "    #Training model\n",
    "    model.train()\n",
    "    for images,labels in train_data:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images=images.view(32,-1) #flattening the image\n",
    "        \n",
    "#         images= images.type(torch.float)\n",
    "#         labels= labels.type(torch.float)\n",
    "        \n",
    "        images.to(device) #uploading images and labels to the device\n",
    "        labels.to(device)\n",
    "        \n",
    "        output=model(images) #output is our model predictions\n",
    "        loss=criterion(output,labels) #calculating the loss\n",
    "        \n",
    "        loss.backward() #applying backward propagation\n",
    "        \n",
    "        optimizer.step() #updating the parameters(Gradient Descent)\n",
    "#         scheduler.step()\n",
    "        \n",
    "        train_loss+=loss\n",
    "    training_loss_epoch.append(train_loss)\n",
    "    \n",
    "    #Evaluation Phase\n",
    "    model.eval() #no drop out here \n",
    "    size=len(test_data.dataset)\n",
    "    \n",
    "    with torch.no_grad(): #context manager why because we wont apply gradients in validation phase\n",
    "        no_of_batches=size//batch_size\n",
    "        total_val_loss=0\n",
    "        correct_pred = 0\n",
    "        for images,labels in test_data:\n",
    "            \n",
    "            images=images.view(32,-1) #flattening the image\n",
    "        \n",
    "#             images=images.type(torch.float)\n",
    "#             labels=labels.type(torch.float)\n",
    "        \n",
    "            images.to(device) #uploading images and labels to the device\n",
    "            labels.to(device)\n",
    "            \n",
    "            logits=model(images)\n",
    "            val_loss_=criterion(logits,labels)\n",
    "            \n",
    "            val_loss+=val_loss_\n",
    "            \n",
    "            pred = torch.max(logits,1).indices\n",
    "            \n",
    "            cnt = torch.eq(labels,pred).sum().item()\n",
    "\n",
    "            \n",
    "            correct_pred += cnt\n",
    "            \n",
    "    total_corrects += correct_pred        \n",
    "    \n",
    "            \n",
    "    print(f\"Running the epoch{i}/{epochs}\")\n",
    "    #print('\\n')\n",
    "    print('__' * 20)\n",
    "    print(f'Training loss: {train_loss}')\n",
    "    print(f'Validation loss: {val_loss}')\n",
    "    print(f'Accuracy score: {total_corrects/size}\\n')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
